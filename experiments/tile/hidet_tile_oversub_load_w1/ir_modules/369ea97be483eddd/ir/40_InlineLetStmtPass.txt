def cuda_ld_global_nc_L2_128B_v4_b32(
    addr: void*,
    v0: void*,
    v1: void*,
    v2: void*,
    v3: void*
)
    # kind: cuda_internal
    # func_name: cuda_ld_global_nc_L2_128B_v4_b32
    # cuda.block_dim: 32
    # cuda.grid_dim: 1
    asm volatile ("ld.global.nc.L2::128B.v4.b32 {%0, %1, %2, %3}, [%4];" : "=r"(*cast(uint32*, v0)), "=r"(*cast(uint32*, v1)), "=r"(*cast(uint32*, v2)), "=r"(*cast(uint32*, v3)) : "l"(addr));

def cuda_dynamic_shared_memory_void(
    byte_offset: int32
) -> void*:
    # kind: cuda_internal
    # func_name: cuda_dynamic_shared_memory_void
    # cuda.block_dim: 32
    # cuda.grid_dim: 1
    declare dynamic_smem: uint8*
    return cast(void*, &dynamic_smem[byte_offset])

def cuda_st_global_b32(
    addr_0: void*,
    v0_0: void*
)
    # kind: cuda_internal
    # func_name: cuda_st_global_b32
    # cuda.block_dim: 32
    # cuda.grid_dim: 1
    asm volatile ("st.global.b32 [%0], %1;" :  : "l"(addr_0), "r"(*cast(uint32*, v0_0)));

def over_sub_load(
    a: float32*,
    b: float32*,
    c: float32*
)
    # kind: cuda_kernel
    # cuda.block_dim: 32
    # cuda.grid_dim: 1
    # cuda.dynamic_smem_bytes: 8580
    declare cvt: tensor(float32*, [4])
    for i in range(4):
        cvt[i] = (a + (((threadIdx.x % 8) * 4) + i))
    declare load: tensor(float32, [4])
    cuda_ld_global_nc_L2_128B_v4_b32(cvt[0], &load[0], &load[1], &load[2], &load[3])
    declare cvt_0: tensor(float32*, [4])
    for i_0 in range(4):
        cvt_0[i_0] = (b + (((threadIdx.x % 16) * 4) + i_0))
    declare load_0: tensor(float32, [4])
    cuda_ld_global_nc_L2_128B_v4_b32(cvt_0[0], &load_0[0], &load_0[1], &load_0[2], &load_0[3])
    declare cvt_1: tensor(float32, [2])
    let cvt_smem: tensor_pointer(float32, [65]) = cast(float32*, cuda_dynamic_shared_memory_void(0))
    cuda_syncthreads()
    for i_1 in range(4):
        if (threadIdx.x < 16)
            cvt_smem[(((threadIdx.x % 16) * 4) + i_1)] = load_0[i_1]
    cuda_syncthreads()
    cvt_1[0] = cvt_smem[threadIdx.x]
    cvt_1[1] = cvt_smem[(32 + threadIdx.x)]
    cuda_syncthreads()
    declare expand_dims: tensor(float32, [2])
    expand_dims[0] = cvt_1[0]
    expand_dims[1] = cvt_1[1]
    declare cvt_2: tensor(float32, [64])
    let cvt_smem_0: tensor_pointer(float32, [65, 2]) = cast(float32*, cuda_dynamic_shared_memory_void(0))
    cuda_syncthreads()
    if true
        cvt_smem_0[(threadIdx.x * 2)] = expand_dims[0]
    if true
        cvt_smem_0[(64 + (threadIdx.x * 2))] = expand_dims[1]
    cuda_syncthreads()
    for i_2 in range(32):
        cvt_2[i_2] = cvt_smem_0[((threadIdx.x * 2) + i_2)]
    for i_3 in range(32):
        cvt_2[(32 + i_3)] = cvt_smem_0[(((threadIdx.x * 2) + i_3) + 64)]
    cuda_syncthreads()
    declare broadcast: tensor(float32, [64])
    for i_4 in range(32):
        broadcast[i_4] = cvt_2[0]
    for i_5 in range(32):
        broadcast[(32 + i_5)] = cvt_2[32]
    declare cvt_3: tensor(float32, [1])
    let cvt_smem_1: tensor_pointer(float32, [33]) = cast(float32*, cuda_dynamic_shared_memory_void(0))
    cuda_syncthreads()
    for i_6 in range(4):
        if (threadIdx.x < 8)
            cvt_smem_1[(((threadIdx.x % 8) * 4) + i_6)] = load[i_6]
    cuda_syncthreads()
    cvt_3[0] = cvt_smem_1[threadIdx.x]
    cuda_syncthreads()
    declare expand_dims_0: tensor(float32, [1])
    expand_dims_0[0] = cvt_3[0]
    declare cvt_4: tensor(float32, [64])
    let cvt_smem_2: tensor_pointer(float32, [2, 33]) = cast(float32*, cuda_dynamic_shared_memory_void(0))
    cuda_syncthreads()
    if true
        cvt_smem_2[threadIdx.x] = expand_dims_0[0]
    cuda_syncthreads()
    for i_7 in range(64):
        cvt_4[i_7] = cvt_smem_2[((i_7 * 33) + threadIdx.x)]
    cuda_syncthreads()
    declare broadcast_0: tensor(float32, [64])
    for i_8 in range(64):
        broadcast_0[i_8] = cvt_4[0]
    declare cvt_5: tensor(float32, [64])
    let cvt_smem_3: tensor_pointer(float32, [65, 33]) = cast(float32*, cuda_dynamic_shared_memory_void(0))
    cuda_syncthreads()
    for i_9 in range(64):
        if true
            cvt_smem_3[((i_9 * 33) + threadIdx.x)] = broadcast_0[i_9]
    cuda_syncthreads()
    for i_10 in range(32):
        cvt_5[i_10] = cvt_smem_3[((threadIdx.x * 33) + i_10)]
    for i_11 in range(32):
        cvt_5[(32 + i_11)] = cvt_smem_3[(((threadIdx.x * 33) + i_11) + 1056)]
    cuda_syncthreads()
    declare cres: tensor(float32, [64])
    for i_12 in range(32):
        cres[i_12] = (broadcast[i_12] + cvt_5[i_12])
    for i_13 in range(32):
        cres[(32 + i_13)] = (broadcast[(32 + i_13)] + cvt_5[(32 + i_13)])
    declare cvt_6: tensor(float32*, [64])
    for i_14 in range(32):
        cvt_6[i_14] = (c + ((threadIdx.x * 32) + i_14))
    for i_15 in range(32):
        cvt_6[(32 + i_15)] = ((((threadIdx.x * 32) + i_15) + c) + 1024)
    for i_16 in range(32):
        if true
            cuda_st_global_b32(cvt_6[i_16], &cres[i_16])
    for i_17 in range(32):
        if true
            cuda_st_global_b32(cvt_6[(32 + i_17)], &cres[(32 + i_17)])

def launch(
    a_0: float32*,
    b_0: float32*,
    c_0: float32*
)
    # kind: public
    # label: 
    if false
        cudaFuncSetAttribute(over_sub_load, cudaFuncAttributeMaxDynamicSharedMemorySize, 8580);
    if true
        if false
            printf("Launching kernel with grid_dim = (%d, %d, %d), block_dim = (%d, %d, %d)\n", 1, 1, 1, 32, 1, 1);
            assert(false, 'Invalid launch configuration')
        if false
            cudaFuncSetAttribute(over_sub_load, cudaFuncAttributeMaxDynamicSharedMemorySize, 8580);
            {cudaError_t err = cudaGetLastError(); if (err != cudaSuccess) LOG(ERROR) << "CUDA error: " << cudaGetErrorString(err) << "\n";}
        over_sub_load<<<dim3(1, 1, 1), dim3(32, 1, 1), 8580>>>(a_0, b_0, c_0);
        {cudaError_t err = cudaGetLastError(); if (err != cudaSuccess) LOG(ERROR) << "CUDA error: " << cudaGetErrorString(err) << "\n";}

