def over_sub_load(
    a: float32*,
    b: float32*,
    c: float32*
)
    # kind: cuda_kernel
    # cuda.block_dim: 128
    # cuda.grid_dim: 1
    # cuda.dynamic_smem_bytes: 8580
    declare cvt: tensor(float32*, [4])
    for i in range(4):
        cvt[i] = (a + (((threadIdx.x % 8) * 4) + i))
    declare load: tensor(float32, [4])
    for i_0 in range(1):  # u+
        cuda_ld_global_nc_L2_128B_v4_b32(cvt[0], &load[0], &load[1], &load[2], &load[3])
    declare cvt_0: tensor(float32*, [4])
    for i_1 in range(4):
        cvt_0[i_1] = (b + (((threadIdx.x % 16) * 4) + i_1))
    declare load_0: tensor(float32, [4])
    for i_2 in range(1):  # u+
        cuda_ld_global_nc_L2_128B_v4_b32(cvt_0[0], &load_0[0], &load_0[1], &load_0[2], &load_0[3])
    declare cvt_1: tensor(float32, [1])
    let cvt_smem: tensor_pointer(float32, [65]) = cast(float32*, cast(void*, cuda_dynamic_shared_memory_void(0)))
    cuda_syncthreads()
    for i_3 in range(4):
        if (((threadIdx.x / 32) < 1) && ((threadIdx.x % 32) < 16))
            cvt_smem[(((threadIdx.x % 16) * 4) + i_3)] = load_0[i_3]
    cuda_syncthreads()
    for i_4 in range(1):  # u+
        for i_5 in range(1):  # u+
            cvt_1[0] = cvt_smem[(((threadIdx.x / 64) * 32) + (threadIdx.x % 32))]
    cuda_syncthreads()
    declare expand_dims: tensor(float32, [1])
    for i_6 in range(1):  # u+
        for i_7 in range(1):  # u+
            expand_dims[0] = cvt_1[0]
    declare cvt_2: tensor(float32, [16])
    let cvt_smem_0: tensor_pointer(float32, [65, 2]) = cast(float32*, cast(void*, cuda_dynamic_shared_memory_void(0)))
    cuda_syncthreads()
    for i_8 in range(1):  # u+
        for i_9 in range(1):  # u+
            if (((threadIdx.x / 32) % 2) < 1)
                cvt_smem_0[((((threadIdx.x / 64) * 32) + (threadIdx.x % 32)) * 2)] = expand_dims[0]
    cuda_syncthreads()
    for i_10 in range(1):  # u+
        for i_11 in range(16):
            cvt_2[i_11] = cvt_smem_0[(((((threadIdx.x / 64) * 32) + (threadIdx.x % 32)) * 2) + ((i_11 * 2) + ((threadIdx.x / 32) % 2)))]
    cuda_syncthreads()
    declare broadcast: tensor(float32, [16])
    for i_12 in range(1):  # u+
        for i_13 in range(16):
            broadcast[i_13] = cvt_2[0]
    declare cvt_3: tensor(float32, [1])
    let cvt_smem_1: tensor_pointer(float32, [33]) = cast(float32*, cast(void*, cuda_dynamic_shared_memory_void(0)))
    cuda_syncthreads()
    for i_14 in range(4):
        if (((threadIdx.x / 32) < 1) && ((threadIdx.x % 32) < 8))
            cvt_smem_1[(((threadIdx.x % 8) * 4) + i_14)] = load[i_14]
    cuda_syncthreads()
    for i_15 in range(1):  # u+
        for i_16 in range(1):  # u+
            cvt_3[0] = cvt_smem_1[(threadIdx.x % 32)]
    cuda_syncthreads()
    declare expand_dims_0: tensor(float32, [1])
    for i_17 in range(1):  # u+
        for i_18 in range(1):  # u+
            expand_dims_0[0] = cvt_3[0]
    declare cvt_4: tensor(float32, [32])
    let cvt_smem_2: tensor_pointer(float32, [2, 33]) = cast(float32*, cast(void*, cuda_dynamic_shared_memory_void(0)))
    cuda_syncthreads()
    for i_19 in range(1):  # u+
        for i_20 in range(1):  # u+
            if (((threadIdx.x / 64) < 1) && (((threadIdx.x / 32) % 2) < 1))
                cvt_smem_2[(threadIdx.x % 32)] = expand_dims_0[0]
    cuda_syncthreads()
    for i_21 in range(32):
        for i_22 in range(1):  # u+
            cvt_4[i_21] = cvt_smem_2[((((i_21 * 2) + (threadIdx.x / 64)) * 33) + (threadIdx.x % 32))]
    cuda_syncthreads()
    declare broadcast_0: tensor(float32, [32])
    for i_23 in range(32):
        for i_24 in range(1):  # u+
            broadcast_0[i_23] = cvt_4[0]
    declare cvt_5: tensor(float32, [16])
    let cvt_smem_3: tensor_pointer(float32, [65, 33]) = cast(float32*, cast(void*, cuda_dynamic_shared_memory_void(0)))
    cuda_syncthreads()
    for i_25 in range(32):
        for i_26 in range(1):  # u+
            if (((threadIdx.x / 32) % 2) < 1)
                cvt_smem_3[((((i_25 * 2) + (threadIdx.x / 64)) * 33) + (threadIdx.x % 32))] = broadcast_0[i_25]
    cuda_syncthreads()
    for i_27 in range(1):  # u+
        for i_28 in range(16):
            cvt_5[i_28] = cvt_smem_3[(((((threadIdx.x / 64) * 32) + (threadIdx.x % 32)) * 33) + ((i_28 * 2) + ((threadIdx.x / 32) % 2)))]
    cuda_syncthreads()
    declare cres: tensor(float32, [16])
    for i_29 in range(1):  # u+
        for i_30 in range(16):
            cres[i_30] = (broadcast[i_30] + cvt_5[i_30])
    declare cvt_6: tensor(float32*, [16])
    for i_31 in range(1):  # u+
        for i_32 in range(16):
            cvt_6[i_32] = (c + (((((threadIdx.x / 64) * 32) + (threadIdx.x % 32)) * 32) + ((i_32 * 2) + ((threadIdx.x / 32) % 2))))
    for i_33 in range(1):  # u+
        for i_34 in range(16):
            if true
                cuda_st_global_b32(cvt_6[i_34], &cres[i_34])

def launch(
    a_0: float32*,
    b_0: float32*,
    c_0: float32*
)
    # kind: public
    # label: 
    if false
        cudaFuncSetAttribute(over_sub_load, cudaFuncAttributeMaxDynamicSharedMemorySize, 8580);
    over_sub_load<<<dim3(1, 1, 1), dim3(128, 1, 1), 8580>>>(a_0, b_0, c_0);

