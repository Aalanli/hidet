def cuda_ld_global_nc_L2_128B_v4_b32(
    addr: void*,
    v0: void*,
    v1: void*,
    v2: void*,
    v3: void*
)
    # kind: cuda_internal
    # func_name: cuda_ld_global_nc_L2_128B_v4_b32
    # cuda.block_dim: 128
    # cuda.grid_dim: 1
    asm volatile ("ld.global.nc.L2::128B.v4.b32 {%0, %1, %2, %3}, [%4];" : "=r"(*cast(uint32*, v0)), "=r"(*cast(uint32*, v1)), "=r"(*cast(uint32*, v2)), "=r"(*cast(uint32*, v3)) : "l"(addr));

def cuda_dynamic_shared_memory_void(
    byte_offset: int32
) -> void*:
    # kind: cuda_internal
    # func_name: cuda_dynamic_shared_memory_void
    # cuda.block_dim: 128
    # cuda.grid_dim: 1
    declare dynamic_smem: uint8*
    return cast(void*, &dynamic_smem[byte_offset])

def cuda_st_global_v4_b32(
    addr_0: void*,
    v0_0: void*,
    v1_0: void*,
    v2_0: void*,
    v3_0: void*
)
    # kind: cuda_internal
    # func_name: cuda_st_global_v4_b32
    # cuda.block_dim: 128
    # cuda.grid_dim: 1
    asm volatile ("st.global.v4.b32 [%0], {%1, %2, %3, %4};" :  : "l"(addr_0), "r"(*cast(uint32*, v0_0)), "r"(*cast(uint32*, v1_0)), "r"(*cast(uint32*, v2_0)), "r"(*cast(uint32*, v3_0)));

def over_sub_load(
    a: float32*,
    b: float32*,
    c: float32*
)
    # kind: cuda_kernel
    # cuda.block_dim: 128
    # cuda.grid_dim: 1
    # cuda.dynamic_smem_bytes: 8580
    declare cvt: tensor(float32*, [16])
    for i in range(4):
        for i_0 in range(4):
            cvt[((i * 4) + i_0)] = (a + ((((((i * 4) + (threadIdx.x / 32)) * 4) + ((threadIdx.x % 32) / 8)) * 32) + (((threadIdx.x % 8) * 4) + i_0)))
    declare load: tensor(float32, [16])
    for i_1 in range(4):
        cuda_ld_global_nc_L2_128B_v4_b32(cvt[(i_1 * 4)], &load[(i_1 * 4)], &load[((i_1 * 4) + 1)], &load[((i_1 * 4) + 2)], &load[((i_1 * 4) + 3)])
    declare a1: tensor(float32, [16])
    let cvt_smem: tensor_pointer(float32, [65, 33]) = cast(float32*, cuda_dynamic_shared_memory_void(0))
    cuda_syncthreads()
    for i_2 in range(4):
        for i_3 in range(4):
            if true
                cvt_smem[((((((i_2 * 4) + (threadIdx.x / 32)) * 4) + ((threadIdx.x % 32) / 8)) * 33) + (((threadIdx.x % 8) * 4) + i_3))] = load[((i_2 * 4) + i_3)]
    cuda_syncthreads()
    for i_4 in range(16):
        a1[i_4] = cvt_smem[((((i_4 * 4) + (threadIdx.x / 32)) * 33) + (threadIdx.x % 32))]
    cuda_syncthreads()
    declare cvt_0: tensor(float32*, [8])
    for i_5 in range(4):
        cvt_0[i_5] = (b + (((((threadIdx.x / 32) * 8) + ((threadIdx.x % 32) / 4)) * 16) + (((threadIdx.x % 4) * 4) + i_5)))
    for i_6 in range(4):
        cvt_0[(4 + i_6)] = (b + (((((4 + (threadIdx.x / 32)) * 8) + ((threadIdx.x % 32) / 4)) * 16) + (((threadIdx.x % 4) * 4) + i_6)))
    declare load_0: tensor(float32, [8])
    cuda_ld_global_nc_L2_128B_v4_b32(cvt_0[0], &load_0[0], &load_0[1], &load_0[2], &load_0[3])
    cuda_ld_global_nc_L2_128B_v4_b32(cvt_0[4], &load_0[4], &load_0[5], &load_0[6], &load_0[7])
    declare reduce_op: tensor(float32, [16])
    for i_7 in range(16):
        reduce_op[i_7] = 0.0f
        reduce_op[i_7] = (reduce_op[i_7] + a1[i_7])
    for i_8 in range(16):
        for i_9 in range(5):
            reduce_op[i_8] = (reduce_op[i_8] + cuda_shfl_down_sync(4294967295, reduce_op[i_8], (1 << i_9), 32))
    for i_10 in range(16):
        for i_11 in range(5):
            reduce_op[i_10] = cuda_shfl_up_sync(4294967295, reduce_op[i_10], (1 << ((5 - i_11) - 1)), 32)
        reduce_op[i_10] = reduce_op[i_10]
    declare cvt_1: tensor(float32, [1])
    let cvt_smem_0: tensor_pointer(float32, [65]) = cast(float32*, cuda_dynamic_shared_memory_void(0))
    cuda_syncthreads()
    for i_12 in range(16):
        if ((threadIdx.x % 32) < 1)
            cvt_smem_0[((i_12 * 4) + (threadIdx.x / 32))] = reduce_op[i_12]
    cuda_syncthreads()
    cvt_1[0] = cvt_smem_0[(((threadIdx.x / 64) * 32) + (threadIdx.x % 32))]
    cuda_syncthreads()
    declare expand_dims: tensor(float32, [1])
    expand_dims[0] = cvt_1[0]
    declare cvt_2: tensor(float32, [8])
    let cvt_smem_1: tensor_pointer(float32, [65, 2]) = cast(float32*, cuda_dynamic_shared_memory_void(0))
    cuda_syncthreads()
    if (((threadIdx.x / 32) % 2) < 1)
        cvt_smem_1[((((threadIdx.x / 64) * 32) + (threadIdx.x % 32)) * 2)] = expand_dims[0]
    cuda_syncthreads()
    for i_13 in range(8):
        cvt_2[i_13] = cvt_smem_1[(((((threadIdx.x / 64) * 32) + (threadIdx.x % 32)) * 2) + ((i_13 * 2) + ((threadIdx.x / 32) % 2)))]
    cuda_syncthreads()
    declare broadcast: tensor(float32, [8])
    for i_14 in range(8):
        broadcast[i_14] = cvt_2[0]
    declare cvt_3: tensor(float32*, [8])
    for i_15 in range(4):
        cvt_3[i_15] = (c + (((((threadIdx.x / 32) * 8) + ((threadIdx.x % 32) / 4)) * 16) + (((threadIdx.x % 4) * 4) + i_15)))
    for i_16 in range(4):
        cvt_3[(4 + i_16)] = (c + (((((4 + (threadIdx.x / 32)) * 8) + ((threadIdx.x % 32) / 4)) * 16) + (((threadIdx.x % 4) * 4) + i_16)))
    declare cvt_4: tensor(float32, [8])
    let cvt_smem_2: tensor_pointer(float32, [65, 17]) = cast(float32*, cuda_dynamic_shared_memory_void(0))
    cuda_syncthreads()
    for i_17 in range(8):
        if true
            cvt_smem_2[(((((threadIdx.x / 64) * 32) + (threadIdx.x % 32)) * 17) + ((i_17 * 2) + ((threadIdx.x / 32) % 2)))] = broadcast[i_17]
    cuda_syncthreads()
    for i_18 in range(4):
        cvt_4[i_18] = cvt_smem_2[(((((threadIdx.x / 32) * 8) + ((threadIdx.x % 32) / 4)) * 17) + (((threadIdx.x % 4) * 4) + i_18))]
    for i_19 in range(4):
        cvt_4[(4 + i_19)] = cvt_smem_2[(((((4 + (threadIdx.x / 32)) * 8) + ((threadIdx.x % 32) / 4)) * 17) + (((threadIdx.x % 4) * 4) + i_19))]
    cuda_syncthreads()
    declare cvt_5: tensor(float32, [8])
    for i_20 in range(4):
        cvt_5[i_20] = (load_0[i_20] * cvt_4[i_20])
    for i_21 in range(4):
        cvt_5[(4 + i_21)] = (load_0[(4 + i_21)] * cvt_4[(4 + i_21)])
    if true
        cuda_st_global_v4_b32(cvt_3[0], &cvt_5[0], &cvt_5[1], &cvt_5[2], &cvt_5[3])
    if true
        cuda_st_global_v4_b32(cvt_3[4], &cvt_5[4], &cvt_5[5], &cvt_5[6], &cvt_5[7])

def launch(
    a_0: float32*,
    b_0: float32*,
    c_0: float32*
)
    # kind: public
    # label: 
    if false
        cudaFuncSetAttribute(over_sub_load, cudaFuncAttributeMaxDynamicSharedMemorySize, 8580);
    if true
        if false
            printf("Launching kernel with grid_dim = (%d, %d, %d), block_dim = (%d, %d, %d)\n", 1, 1, 1, 128, 1, 1);
            assert(false, 'Invalid launch configuration')
        if false
            cudaFuncSetAttribute(over_sub_load, cudaFuncAttributeMaxDynamicSharedMemorySize, 8580);
            {cudaError_t err = cudaGetLastError(); if (err != cudaSuccess) LOG(ERROR) << "CUDA error: " << cudaGetErrorString(err) << "\n";}
        over_sub_load<<<dim3(1, 1, 1), dim3(128, 1, 1), 8580>>>(a_0, b_0, c_0);
        {cudaError_t err = cudaGetLastError(); if (err != cudaSuccess) LOG(ERROR) << "CUDA error: " << cudaGetErrorString(err) << "\n";}

